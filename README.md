Local RAG Chatbot (Streamlit + LangChain + Chroma + Ollama / Llama 3)
Overview

This project is a local Retrieval-Augmented Generation (RAG) chatbot that lets you upload PDFs, build a knowledge base, and query them conversationally — all fully offline. It combines:

Ollama (LLaMA 3) → Runs a modern large language model locally on your machine.

LangChain → Provides the orchestration to connect the LLM with document retrieval and memory.

Chroma → Vector database used to store and retrieve document embeddings.

Streamlit → Interactive web app frontend for uploading PDFs and chatting.

What is RAG?

Retrieval-Augmented Generation (RAG) is a technique where an LLM is combined with an external knowledge source (like PDFs or a database). Instead of relying only on what the model was trained on, it retrieves relevant context chunks from your documents and uses them to generate accurate, up-to-date answers.

Why Ollama 3 + LangChain?

Ollama 3 → Brings Meta’s LLaMA 3 model to your laptop/PC, no cloud or API keys needed.

LangChain → Simplifies building RAG pipelines by handling embeddings, vector stores, and query routing.

What does Streamlit do?

Provides a simple, interactive web UI to:

Upload PDFs

Build a knowledge base (store embeddings in Chroma)

Ask natural language questions

Streamlit makes it possible to run the entire chatbot in a browser with just one Python script.

Quick Start
1) Create virtual environment & install dependencies
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

2) Install & start Ollama
curl -fsSL https://ollama.com/install.sh | sh
ollama serve

3) Pull models (first time only)
ollama pull llama3
ollama pull nomic-embed-text

4) Run app
streamlit run app.py

Usage

Upload one or more PDFs (e.g., sample_5_lines.pdf included).

Click "Build knowledge base" to process and embed documents.

Ask natural language questions — responses are generated by LLaMA 3 with retrieved context.

Works fully offline, no API keys required.
